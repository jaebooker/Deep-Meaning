{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda, Input\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdb = pd.read_csv(\"Shakespeare-plays/Shakespeare_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['comedy','tragedy','other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_the_plays():\n",
    "    new_play = \"Henry IV\"\n",
    "    play_array = []\n",
    "    for i in sdb['Play']:\n",
    "        if i != new_play:\n",
    "            play_array.append(new_play)\n",
    "            new_play = i\n",
    "    return play_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plays = all_the_plays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(play_name):\n",
    "    play = sdb[sdb['Play']==play_name]\n",
    "    w = play['PlayerLine']\n",
    "    text = \"\"\n",
    "    for i in w:\n",
    "        text += i\n",
    "        text += \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_training = []\n",
    "tragedy_training = []\n",
    "history_training = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_text():\n",
    "    comedy_training.append(get_text(\"Alls well that ends well\"))\n",
    "    comedy_training.append(get_text(\"As you like it\"))\n",
    "    comedy_training.append(get_text(\"Loves Labours Lost\"))\n",
    "    comedy_training.append(get_text(\"Measure for measure\"))\n",
    "    comedy_training.append(get_text(\"Merry Wives of Windsor\"))\n",
    "    comedy_training.append(get_text(\"A Midsummer nights dream\"))\n",
    "    comedy_training.append(get_text(\"Taming of the Shrew\"))\n",
    "    comedy_training.append(get_text(\"The Tempest\"))\n",
    "    comedy_training.append(get_text(\"Two Gentlemen of Verona\"))\n",
    "    comedy_training.append(get_text(\"A Winters Tale\"))\n",
    "    tragedy_training.append(get_text(\"Antony and Cleopatra\"))\n",
    "    tragedy_training.append(get_text(\"Coriolanus\"))\n",
    "    tragedy_training.append(get_text(\"Hamlet\"))\n",
    "    tragedy_training.append(get_text(\"Julius Caesar\"))\n",
    "    tragedy_training.append(get_text(\"macbeth\"))\n",
    "    tragedy_training.append(get_text(\"Othello\"))\n",
    "    tragedy_training.append(get_text(\"Timon of Athens\"))\n",
    "    tragedy_training.append(get_text(\"Titus Andronicus\"))\n",
    "    history_training.append(get_text(\"Henry IV\"))\n",
    "    history_training.append(get_text(\"Henry VI Part 1\"))\n",
    "    history_training.append(get_text(\"Henry VI Part 3\"))\n",
    "    history_training.append(get_text(\"Henry VIII\"))\n",
    "    history_training.append(get_text(\"King John\"))\n",
    "    history_training.append(get_text(\"Richard II\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_testing = []\n",
    "tragedy_testing = []\n",
    "history_testing = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_text():\n",
    "    comedy_testing.append(get_text(\"A Comedy of Errors\"))\n",
    "    comedy_testing.append(get_text(\"Merchant of Venice\"))\n",
    "    comedy_testing.append(get_text(\"Much Ado about nothing\"))\n",
    "    comedy_testing.append(get_text(\"Twelfth Night\"))\n",
    "    tragedy_testing.append(get_text(\"Cymbeline\"))\n",
    "    tragedy_testing.append(get_text(\"King Lear\"))\n",
    "    tragedy_testing.append(get_text(\"Romeo and Juliet\"))\n",
    "    tragedy_testing.append(get_text(\"Troilus and Cressida\"))\n",
    "    history_testing.append(get_text(\"Henry V\"))\n",
    "    history_testing.append(get_text(\"Henry VI Part 3\"))\n",
    "    history_testing.append(get_text(\"Pericles\"))\n",
    "    history_testing.append(get_text(\"Richard III\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_training_text()\n",
    "get_testing_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shakes_text_array(text):\n",
    "    w = re.split(\"\\W*[^\\'\\w+\\']\", text)\n",
    "    text_array = []\n",
    "    for i in w:\n",
    "        text_array.append([i])\n",
    "    return text_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(max_sequence_len, total_words):\n",
    "#     input_len = max_sequence_len - 1\n",
    "#     model = Sequential()\n",
    "    \n",
    "#     # Add Input Embedding Layer\n",
    "#     model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "#     # Add Hidden Layer 1 - LSTM Layer\n",
    "#     model.add(LSTM(512))\n",
    "#     model.add(Dropout(0.4))\n",
    "    \n",
    "#     # Add Output Layer\n",
    "#     model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# model = create_model(max_sequence_len, total_words)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(words):\n",
    "    count_words = Counter(words)\n",
    "    total_words = len(words)\n",
    "    sorted_words = count_words.most_common(total_words)\n",
    "    return sorted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_encoding(words):\n",
    "    words_split = words.split('\\n')\n",
    "    words_int = []\n",
    "    for play in words_split:\n",
    "        p = [words_int[w] for w in play.split()]\n",
    "        words_int.append(p)\n",
    "    return words_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(labels_split):\n",
    "    encoded_labels = []\n",
    "    for l in labels_split:\n",
    "        if l == 'comedy':\n",
    "            encoded_labels.append(0)\n",
    "        elif l == 'tragedy':\n",
    "            encoded_labels.append(1)\n",
    "        else:\n",
    "            encoded_labels.append(2)\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(encoded_labels) = encoding(labels_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(plays_int, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(plays_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, p in enumerate(plays_int):\n",
    "        play_len = len(p)\n",
    "        \n",
    "        if play_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-play_len))\n",
    "            new = zeroes+p\n",
    "        elif play_len > seq_length:\n",
    "            new = play[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split():\n",
    "    split_frac = 0.8\n",
    "    train_x = features[0:int(split_frac*len_feat)]\n",
    "    train_y = encoded_labels[0:int(split_frac*len_feat)]\n",
    "    remaining_x = features[int(split_frac*len_feat):]\n",
    "    remaining_y = encoded_labels[int(split_frac*len_feat):]\n",
    "    valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
    "    valid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
    "    test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
    "    test_y = remaining_y[int(len(remaining_y)*0.5):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_datasets():\n",
    "    # create Tensor datasets\n",
    "    train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "    valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "    test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "    # dataloaders\n",
    "    batch_size = 50\n",
    "    # make sure to SHUFFLE your data\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Class thanks to Samarth Agrawal\"\"\"\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_with_params():\n",
    "#     # Instantiate the model w/ hyperparams\n",
    "#     vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "#     output_size = 1\n",
    "#     embedding_dim = 400\n",
    "#     hidden_dim = 256\n",
    "#     n_layers = 2\n",
    "#     net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "#     print(net)\n",
    "#     SentimentRNN(\n",
    "#       (embedding): Embedding(74073, 400)\n",
    "#       (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
    "#       (dropout): Dropout(p=0.3)\n",
    "#       (fc): Linear(in_features=256, out_features=1, bias=True)\n",
    "#       (sig): Sigmoid()\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    # loss and optimization functions\n",
    "    lr=0.001\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    # training params\n",
    "\n",
    "    epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "    counter = 0\n",
    "    print_every = 100\n",
    "    clip=5 # gradient clipping\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    net.train()\n",
    "    # train for some number of epochs\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            inputs = inputs.type(torch.LongTensor)\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "\n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    inputs = inputs.type(torch.LongTensor)\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
