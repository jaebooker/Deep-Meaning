{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda, Input, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from string import punctuation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdb = pd.read_csv(\"Shakespeare-plays/Shakespeare_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['comedy','tragedy','other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_the_plays():\n",
    "    new_play = \"Henry IV\"\n",
    "    play_array = []\n",
    "    for i in sdb['Play']:\n",
    "        if i != new_play:\n",
    "            play_array.append(new_play)\n",
    "            new_play = i\n",
    "    return play_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plays = all_the_plays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(play_name):\n",
    "    play = sdb[sdb['Play']==play_name]\n",
    "    w = play['PlayerLine']\n",
    "    text = \"\"\n",
    "    for i in w:\n",
    "        text += i\n",
    "        text += \" \"\n",
    "    inp_sequences, total_words = get_sequence_of_tokens(text)\n",
    "    inp_sequences[:10]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_training = []\n",
    "tragedy_training = []\n",
    "history_training = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    corpus = corpus[:7000]\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_text():\n",
    "    comedy_training.append(get_text(\"Alls well that ends well\"))\n",
    "    comedy_training.append(get_text(\"As you like it\"))\n",
    "    comedy_training.append(get_text(\"Loves Labours Lost\"))\n",
    "    comedy_training.append(get_text(\"Measure for measure\"))\n",
    "    comedy_training.append(get_text(\"Merry Wives of Windsor\"))\n",
    "    comedy_training.append(get_text(\"A Midsummer nights dream\"))\n",
    "    comedy_training.append(get_text(\"Taming of the Shrew\"))\n",
    "    comedy_training.append(get_text(\"The Tempest\"))\n",
    "    comedy_training.append(get_text(\"Two Gentlemen of Verona\"))\n",
    "    comedy_training.append(get_text(\"A Winters Tale\"))\n",
    "    tragedy_training.append(get_text(\"Antony and Cleopatra\"))\n",
    "    tragedy_training.append(get_text(\"Coriolanus\"))\n",
    "    tragedy_training.append(get_text(\"Hamlet\"))\n",
    "    tragedy_training.append(get_text(\"Julius Caesar\"))\n",
    "    tragedy_training.append(get_text(\"macbeth\"))\n",
    "    tragedy_training.append(get_text(\"Othello\"))\n",
    "    tragedy_training.append(get_text(\"Timon of Athens\"))\n",
    "    tragedy_training.append(get_text(\"Titus Andronicus\"))\n",
    "    history_training.append(get_text(\"Henry IV\"))\n",
    "    history_training.append(get_text(\"Henry VI Part 1\"))\n",
    "    history_training.append(get_text(\"Henry VI Part 3\"))\n",
    "    history_training.append(get_text(\"Henry VIII\"))\n",
    "    history_training.append(get_text(\"King John\"))\n",
    "    history_training.append(get_text(\"Richard II\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_testing = []\n",
    "tragedy_testing = []\n",
    "history_testing = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_text():\n",
    "    comedy_testing.append(get_text(\"A Comedy of Errors\"))\n",
    "    comedy_testing.append(get_text(\"Merchant of Venice\"))\n",
    "    comedy_testing.append(get_text(\"Much Ado about nothing\"))\n",
    "    comedy_testing.append(get_text(\"Twelfth Night\"))\n",
    "    tragedy_testing.append(get_text(\"Cymbeline\"))\n",
    "    tragedy_testing.append(get_text(\"King Lear\"))\n",
    "    tragedy_testing.append(get_text(\"Romeo and Juliet\"))\n",
    "    tragedy_testing.append(get_text(\"Troilus and Cressida\"))\n",
    "    history_testing.append(get_text(\"Henry V\"))\n",
    "    history_testing.append(get_text(\"Henry VI Part 3\"))\n",
    "    history_testing.append(get_text(\"Pericles\"))\n",
    "    history_testing.append(get_text(\"Richard III\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_training_text()\n",
    "get_testing_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shakes_text_array(text):\n",
    "    w = re.split(\"\\W*[^\\'\\w+\\']\", text)\n",
    "    text_array = []\n",
    "    for i in w:\n",
    "        text_array.append([i])\n",
    "    return text_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(max_sequence_len, total_words):\n",
    "#     input_len = max_sequence_len - 1\n",
    "#     model = Sequential()\n",
    "    \n",
    "#     # Add Input Embedding Layer\n",
    "#     model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "#     # Add Hidden Layer 1 - LSTM Layer\n",
    "#     model.add(LSTM(512))\n",
    "#     model.add(Dropout(0.4))\n",
    "    \n",
    "#     # Add Output Layer\n",
    "#     model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# model = create_model(max_sequence_len, total_words)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(words):\n",
    "    count_words = Counter(words)\n",
    "    total_words = len(words)\n",
    "    sorted_words = count_words.most_common(total_words)\n",
    "    return sorted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_encoding(words):\n",
    "    words_split = words.split('\\n')\n",
    "    words_int = []\n",
    "    for play in words_split:\n",
    "        p = [words_int[w] for w in play.split()]\n",
    "        words_int.append(p)\n",
    "    return words_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(labels_split):\n",
    "    encoded_labels = []\n",
    "    for l in labels_split:\n",
    "        if l == 'comedy':\n",
    "            encoded_labels.append(0)\n",
    "        elif l == 'tragedy':\n",
    "            encoded_labels.append(1)\n",
    "        else:\n",
    "            encoded_labels.append(2)\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(encoded_labels) = encoding(labels_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = word_counter(comedy_training[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccc = {w:i+1 for i, (w,c) in enumerate(cc)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, 'e': 2, 't': 3, 'o': 4, 'a': 5, 'h': 6, 's': 7, 'n': 8, 'r': 9, 'i': 10, 'l': 11, 'd': 12, 'u': 13, 'm': 14, ',': 15, 'y': 16, 'w': 17, 'f': 18, 'c': 19, 'g': 20, 'p': 21, 'b': 22, 'v': 23, '.': 24, 'I': 25, 'k': 26, \"'\": 27, 'T': 28, ':': 29, 'A': 30, 'W': 31, 'E': 32, '?': 33, 'S': 34, 'H': 35, '-': 36, 'M': 37, 'F': 38, 'B': 39, 'N': 40, 'L': 41, 'O': 42, '!': 43, 'x': 44, 'R': 45, 'C': 46, 'G': 47, 'Y': 48, 'q': 49, 'P': 50, 'D': 51, 'j': 52, 'U': 53, 'K': 54, 'V': 55, 'z': 56, '[': 57, ']': 58, 'J': 59, 'Q': 60, '\\t': 61}\n"
     ]
    }
   ],
   "source": [
    "print(ccc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(plays_int, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(plays_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, p in enumerate(plays_int):\n",
    "        play_len = len(p)\n",
    "        \n",
    "        if play_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-play_len))\n",
    "            new = zeroes+p\n",
    "        elif play_len > seq_length:\n",
    "            new = play[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split():\n",
    "    split_frac = 0.8\n",
    "    train_x = features[0:int(split_frac*len_feat)]\n",
    "    train_y = encoded_labels[0:int(split_frac*len_feat)]\n",
    "    remaining_x = features[int(split_frac*len_feat):]\n",
    "    remaining_y = encoded_labels[int(split_frac*len_feat):]\n",
    "    valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
    "    valid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
    "    test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
    "    test_y = remaining_y[int(len(remaining_y)*0.5):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_datasets():\n",
    "    # create Tensor datasets\n",
    "    train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "    valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "    test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "    # dataloaders\n",
    "    batch_size = 50\n",
    "    # make sure to SHUFFLE your data\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Class thanks to Samarth Agrawal\"\"\"\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_with_params():\n",
    "#     # Instantiate the model w/ hyperparams\n",
    "#     vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "#     output_size = 1\n",
    "#     embedding_dim = 400\n",
    "#     hidden_dim = 256\n",
    "#     n_layers = 2\n",
    "#     net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "#     print(net)\n",
    "#     SentimentRNN(\n",
    "#       (embedding): Embedding(74073, 400)\n",
    "#       (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
    "#       (dropout): Dropout(p=0.3)\n",
    "#       (fc): Linear(in_features=256, out_features=1, bias=True)\n",
    "#       (sig): Sigmoid()\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    # loss and optimization functions\n",
    "    lr=0.001\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    # training params\n",
    "\n",
    "    epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "    counter = 0\n",
    "    print_every = 100\n",
    "    clip=5 # gradient clipping\n",
    "\n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "\n",
    "    net.train()\n",
    "    # train for some number of epochs\n",
    "                        \n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "\n",
    "        # batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "\n",
    "            # get the output from the model\n",
    "            inputs = inputs.type(torch.LongTensor)\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for inputs, labels in valid_loader:\n",
    "\n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    inputs = inputs.type(torch.LongTensor)\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    \n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                net.train()\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing():\n",
    "    # Get test data loss and accuracy\n",
    "\n",
    "    test_losses = [] # track loss\n",
    "    num_correct = 0\n",
    "\n",
    "    # init hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    net.eval()\n",
    "    # iterate over test data\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # get predicted outputs\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate loss\n",
    "        test_loss = criterion(output.squeeze(), labels.float())\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        # convert output probabilities to predicted class (0 or 1)\n",
    "        pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "\n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "    # -- stats! -- ##\n",
    "    # avg test loss\n",
    "    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "    # accuracy over all test data\n",
    "    test_acc = num_correct/len(test_loader.dataset)\n",
    "    print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_play(test_play):\n",
    "    test_play = test_play.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_play if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
    "\n",
    "    return test_ints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_play, sequence_length=200):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize play\n",
    "    test_ints = tokenize_play(test_play)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "    if(pred.item()==0):\n",
    "        print(\"Tiss a happy play!\")\n",
    "    elif(pred.item()==1):\n",
    "        print(\"Tiss a sad play!\")\n",
    "    else:\n",
    "        print(\"Tiss a play recounting history!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once more, with feeling'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Once more, with feeling\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer():\n",
    "    max_fatures = 2000\n",
    "    tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "    tokenizer.fit_on_texts(data['text'].values)\n",
    "    X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "    X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soon...\n",
    "# embed_dim = 128\n",
    "# lstm_out = 196\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "# model.add(SpatialDropout1D(0.4))\n",
    "# model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(2,activation='softmax'))\n",
    "# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = pd.get_dummies(data['sentiment']).values\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "# print(X_train.shape,Y_train.shape)\n",
    "# print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_size = 1500\n",
    "\n",
    "# X_validate = X_test[-validation_size:]\n",
    "# Y_validate = Y_test[-validation_size:]\n",
    "# X_test = X_test[:-validation_size]\n",
    "# Y_test = Y_test[:-validation_size]\n",
    "# score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "# print(\"score: %.2f\" % (score))\n",
    "# print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letsee():\n",
    "    pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
    "    for x in range(len(X_validate)):\n",
    "\n",
    "        result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "\n",
    "        if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "            if np.argmax(Y_validate[x]) == 0:\n",
    "                neg_correct += 1\n",
    "            else:\n",
    "                pos_correct += 1\n",
    "\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            neg_cnt += 1\n",
    "        else:\n",
    "            pos_cnt += 1\n",
    "\n",
    "\n",
    "\n",
    "    print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n",
    "    print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twt = ['Meetings: Because none of us is as dumb as all of us.']\n",
    "# #vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "# twt = tokenizer.texts_to_sequences(twt)\n",
    "# #padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "# twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n",
    "# print(twt)\n",
    "# sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "# if(np.argmax(sentiment) == 0):\n",
    "#     print(\"negative\")\n",
    "# elif (np.argmax(sentiment) == 1):\n",
    "#     print(\"positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Third time, third time!!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Third time, third time!!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary_size = 5000\n",
    "# (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "# print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = []\n",
    "for i in comedy_training:\n",
    "    tk.fit_on_texts(i)\n",
    "    index_list = tk.texts_to_sequences(i)\n",
    "    train_array.append(pad_sequences(index_list, maxlen=50000))\n",
    "for x in tragedy_training:\n",
    "    train_array.append(x)\n",
    "for z in history_training:\n",
    "    train_array.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = []\n",
    "for i in comedy_testing:\n",
    "    train_array.append(i)\n",
    "for x in tragedy_testing:\n",
    "    train_array.append(x)\n",
    "for z in history_testing:\n",
    "    train_array.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "for i in comedy_training:\n",
    "    train_labels.append(0)\n",
    "for x in tragedy_training:\n",
    "    train_labels.append(1)\n",
    "for z in history_training:\n",
    "    train_labels.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "for i in comedy_testing:\n",
    "    train_labels.append(0)\n",
    "for x in tragedy_testing:\n",
    "    train_labels.append(1)\n",
    "for z in history_testing:\n",
    "    train_labels.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_array\n",
    "X_test = test_array\n",
    "y_train = train_labels\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 5000000\n",
    "vocabulary_size = 5000\n",
    "# X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "# X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 5000000, 32)       160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_size=32\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-a1f9ba31ff6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 'Got tensor with shape: %s' % str(shape))\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "X_valid, y_valid = X_train[:], y_train[:]\n",
    "X_train2, y_train2 = X_train[:], y_train[:]\n",
    "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
